{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U aifactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ceb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install -U opencv-python==4.10.0.82 numpy==1.26.4 scikit-learn==1.3.2 scipy==1.11.4\n",
    "!pip install -U dlib\n",
    "!pip install -U timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkgid\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('data/sample_image_1.png'), WindowsPath('data/sample_image_2.png'), WindowsPath('data/sample_image_3.png'), WindowsPath('data/sample_image_4.png'), WindowsPath('data/sample_image_5.png'), WindowsPath('data/sample_image_6.png'), WindowsPath('data/sample_image_7.png')]\n",
      "Using 8 worker processes for preprocessing.\n",
      "Using device - cuda\n",
      "Loading weights from checkpoints\\dino\\attempt1\\epoch-001.pt...\n",
      "Model successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import csv\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from models import DinoDiscriminator2\n",
    "\n",
    "# Model\n",
    "checkpoint = Path(\"./checkpoints/dino2/attempt1/epoch-004.pt\")\n",
    "img_size = 518\n",
    "threshold = 0.4\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(img_size, v2.InterpolationMode.BICUBIC),\n",
    "        v2.CenterCrop(img_size),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Processing\n",
    "test_dataset_path = Path(\"./data\")\n",
    "files = [p for p in sorted(test_dataset_path.iterdir()) if p.is_file()]\n",
    "print(files)\n",
    "output_csv_path = Path(\"submission.csv\")\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "VIDEO_EXTS = {\".avi\", \".mp4\"}\n",
    "\n",
    "num_workers = min(max(1, multiprocessing.cpu_count() - 1), 8)\n",
    "print(f\"Using {num_workers} worker processes for preprocessing.\")\n",
    "\n",
    "def get_boundingbox(face, width, height):\n",
    "    x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * 1.3)\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "    return x1, y1, size_bb\n",
    "\n",
    "def detect_and_crop_face_optimized(image: Image.Image, resize_for_detection=640):\n",
    "    if image.mode != 'RGB': image = image.convert('RGB')\n",
    "    original_np = np.array(image)\n",
    "    original_h, original_w, _ = original_np.shape\n",
    "    if original_w > resize_for_detection:\n",
    "        scale = resize_for_detection / float(original_w)\n",
    "        resized_h = int(original_h * scale)\n",
    "        resized_np = cv2.resize(original_np, (resize_for_detection, resized_h), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        scale = 1.0\n",
    "        resized_np = original_np\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = face_detector(resized_np, 1)\n",
    "\n",
    "    if not faces: return None\n",
    "    face = max(faces, key=lambda rect: rect.width() * rect.height())\n",
    "    scaled_face_rect = dlib.rectangle(\n",
    "        left=int(face.left() / scale), top=int(face.top() / scale),\n",
    "        right=int(face.right() / scale), bottom=int(face.bottom() / scale)\n",
    "    )\n",
    "    x, y, size = get_boundingbox(scaled_face_rect, original_w, original_h)\n",
    "    cropped_np = original_np[y:y + size, x:x + size]\n",
    "    face_img = Image.fromarray(cropped_np)\n",
    "    return face_img\n",
    "\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"파일 경로를 입력받아 전처리된 얼굴 이미지 리스트와 파일 이름을 반환\"\"\"\n",
    "    print(f\"processing {file_path.name}\")\n",
    "\n",
    "    face_images = []\n",
    "    ext = file_path.suffix.lower()\n",
    "    num_frames_to_extract = 30\n",
    "\n",
    "    try:\n",
    "        if ext in IMAGE_EXTS:\n",
    "            image = Image.open(file_path)\n",
    "            face_img = detect_and_crop_face_optimized(image)\n",
    "            if face_img:\n",
    "                face_images.append(face_img)\n",
    "                \n",
    "        elif ext in VIDEO_EXTS:\n",
    "            cap = cv2.VideoCapture(str(file_path))\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if total_frames > 0:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, num_frames_to_extract, dtype=int)\n",
    "                for idx in frame_indices:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret: continue\n",
    "                    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    face_img = detect_and_crop_face_optimized(image)\n",
    "                    if face_img:\n",
    "                        face_images.append(face_img)\n",
    "            cap.release()\n",
    "    except Exception as e:\n",
    "        return file_path.name, [], str(e)\n",
    "\n",
    "    return file_path.name, face_images, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device - {device}\")\n",
    "    model = DinoDiscriminator2(pretrained=False).to(device)\n",
    "    print(f\"Loading weights from {checkpoint}...\")\n",
    "    model.load(checkpoint, map_location=device)\n",
    "    model.eval()\n",
    "    print(\"Model successfully loaded.\")\n",
    "\n",
    "    results_to_write = {}\n",
    "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
    "        for filename, face_images, error in pool.imap_unordered(process_single_file, files):\n",
    "            if error:\n",
    "                print(f\"Error processing {filename}: {error}\")\n",
    "            \n",
    "            if not face_images:\n",
    "                results_to_write[filename] = 0\n",
    "                continue\n",
    "\n",
    "            tensors = torch.stack([transform(img) for img in face_images]).to(device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                logits = model(tensors) # [N, 1]\n",
    "            probs = torch.sigmoid(logits.view(-1)).mean()\n",
    "            preds = (probs > threshold).int().item()\n",
    "            results_to_write[filename] = preds\n",
    "\n",
    "            print(f\"{filename} - {preds}\")\n",
    "                \n",
    "    print(\"Writing results to CSV...\")\n",
    "    with open(output_csv_path, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"label\"])\n",
    "        for p in files:\n",
    "            filename = p.name\n",
    "            label = results_to_write.get(filename, 0)\n",
    "            writer.writerow([filename, label])\n",
    "\n",
    "    print(\"Inference completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4df34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : task\n",
      "jupyter notebook\n",
      "제출 완료\n",
      "28.774463176727295\n"
     ]
    }
   ],
   "source": [
    "import aifactory.score as aif\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "aif.submit(\n",
    "    model_name=\"dino2_eval-4_threshold-0.4\",\n",
    "    key=\"4fb7354a-6bcb-4442-b77d-e8f93ec1e1e3\",\n",
    ")\n",
    "\n",
    "print(time.time() - t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
